{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoOAFQD9IXzy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.layers import wrappers\n",
    "#from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import pydot_ng as pydot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lkEI2DuIXzz"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVoc0OqyIXz0"
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "Ehv7b7gKIXz1",
    "outputId": "896f8934-0c87-4931-de63-4f873cbd32aa"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "CO2VUGnkIXz2",
    "outputId": "29f105ea-59c9-491c-d362-2f8a85545f37"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jaDFf3sLIXz3",
    "outputId": "2c63856f-fb5c-4a20-bad5-6bfe23fef698"
   },
   "outputs": [],
   "source": [
    "#Mean Subtraction\n",
    "train_images_mean = []\n",
    "mean = np.mean(train_images, axis = 2)\n",
    "for i in range(train_images.shape[0]):\n",
    "    image = []\n",
    "    for j in range(train_images.shape[1]):\n",
    "        image.append(train_images[i][j] - mean[i][j])\n",
    "    train_images_mean.append(image)\n",
    "train_images_mean = np.array(train_images_mean)\n",
    "train_images_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "ae5ljB_YIXz3",
    "outputId": "67ed33d5-d67c-4415-f0fe-d80a3619b5a0"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images_mean[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "2_e6rQ6CIXz4",
    "outputId": "3f5dba78-64fa-42c5-a2b0-3d06cfc9beb4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images_mean[i])\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSzYdOnGIXz6",
    "outputId": "f82efa3d-8423-44b2-9c00-c235fa41effd"
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "train_images_normal = []\n",
    "std = np.std(train_images, axis = 2)\n",
    "for i in range(train_images.shape[0]):\n",
    "    image = []\n",
    "    for j in range(train_images.shape[1]):\n",
    "        if std[i][j] == 0:\n",
    "            image.append(train_images_mean[i][j])\n",
    "        else:\n",
    "            image.append(train_images_mean[i][j] / std[i][j])\n",
    "    train_images_normal.append(image)\n",
    "train_images_normal = np.array(train_images_normal)\n",
    "train_images_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "fl9LEVeRIXz7",
    "outputId": "f3672d8e-1648-427e-e6e2-97eee8950de3"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images_normal[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "EVe5hahoIXz8",
    "outputId": "b4103810-fc3f-4d4f-db12-f80b23ce1303"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images_mean[i])\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeqerzfIIXz8"
   },
   "outputs": [],
   "source": [
    "train_size, train_rows, train_cols = train_images.shape\n",
    "train_images_mean = train_images_mean.reshape(train_size, train_rows*train_cols)\n",
    "train_images_normal = train_images_normal.reshape(train_size, train_rows*train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "et7CA1sMIXz8"
   },
   "outputs": [],
   "source": [
    "cov = np.dot(train_images_mean.T, train_images_mean) / train_images_mean.shape[0]\n",
    "U,S,V = np.linalg.svd(cov)\n",
    "Xrot_train = np.dot(train_images_mean, U)\n",
    "Xrot_train_reduced = np.dot(train_images_mean, U[:, :100])\n",
    "Xrot_t = Xrot_train.reshape(train_size, train_rows, train_cols)\n",
    "Xrot_train_r = Xrot_train_reduced.reshape(train_size, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "85pWOqfDIXz9",
    "outputId": "c9527fa1-4445-4a1f-9d11-0ae8d7cb226b"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(Xrot_train_r[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "gVs4Xm5rIXz-",
    "outputId": "68ae105f-0fa2-4dd6-b3cf-3ff36997d57d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(Xrot_train_r[i])\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0M_dR2vIXz-",
    "outputId": "74465000-53c5-4491-9d74-d86a2234387b"
   },
   "outputs": [],
   "source": [
    "#Mean Subtraction\n",
    "test_images_mean = []\n",
    "mean = np.mean(test_images, axis = 2)\n",
    "for i in range(test_images.shape[0]):\n",
    "    image = []\n",
    "    for j in range(test_images.shape[1]):\n",
    "        image.append(test_images[i][j] - mean[i][j])\n",
    "    test_images_mean.append(image)\n",
    "test_images_mean = np.array(test_images_mean)\n",
    "test_images_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YX8_XoQbIXz_",
    "outputId": "f306f253-4dac-4e28-b448-2db34cb6911c"
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "test_images_normal = []\n",
    "std = np.std(test_images, axis = 2)\n",
    "for i in range(test_images.shape[0]):\n",
    "    image = []\n",
    "    for j in range(test_images.shape[1]):\n",
    "        if std[i][j] == 0:\n",
    "            image.append(test_images_mean[i][j])\n",
    "        else:\n",
    "            image.append(test_images_mean[i][j] / std[i][j])\n",
    "    test_images_normal.append(image)\n",
    "test_images_normal = np.array(test_images_normal)\n",
    "test_images_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBj1S_d9IXz_"
   },
   "outputs": [],
   "source": [
    "test_size, test_rows, test_cols = test_images.shape\n",
    "test_images_mean = test_images_mean.reshape(test_size, test_rows*test_cols)\n",
    "test_images_normal = test_images_normal.reshape(test_size, test_rows*test_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTWAWPibIX0A"
   },
   "outputs": [],
   "source": [
    "cov = np.dot(test_images_mean.T, test_images_mean) / test_images_mean.shape[0]\n",
    "U,S,V = np.linalg.svd(cov)\n",
    "Xrot_test = np.dot(test_images_mean, U)\n",
    "Xrot_test_reduced = np.dot(test_images_mean, U[:, :100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "K3zZBsx0IX0A",
    "outputId": "39147da4-351a-4109-ad7c-00f391174d80"
   },
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "\n",
    "    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def run(self, gradient_fn, x, y, params, biases):\n",
    "        norms = np.array([np.inf])\n",
    "        t = 1\n",
    "        while np.any(norms > self.epsilon) and t < self.max_iters:\n",
    "            grad, bias = gradient_fn(x, y, params)\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "                #biases[p] -= self.learning_rate * bias[p]\n",
    "\n",
    "            t += 1\n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        return params, biases\n",
    "\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, activation, input_units, output_units, bias):\n",
    "        self.activation = activation  #Set the activation function\n",
    "        self.input_units = input_units  #Set number of input units\n",
    "        self.output_units = output_units  #Set number of output units\n",
    "        self.bias = bias  #Set the bias\n",
    "        #self.weights = np.random.randn(self.input_units, self.output_units) * 0.01\n",
    "        self.weights = np.random.uniform(-0.1, 0.1, size=(self.input_units, self.output_units))\n",
    "\n",
    "    #Allows for the weight parameter to be updated with new values\n",
    "    def update_weights(self, new_weights):\n",
    "        self.weights = new_weights\n",
    "\n",
    "    def update_biases(self, new_biases):\n",
    "        self.bias = new_biases\n",
    "\n",
    "    #Calculates the output given input and current weights\n",
    "    def forward(self, input):\n",
    "        #return self.activation(np.add(np.dot(input, self.weights), self.bias))\n",
    "        return self.activation(np.dot(input, self.weights)) # + np.ndarray.flatten(self.bias))\n",
    "\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, activation, activation_prime, hidden, num_units, bias):\n",
    "        self.activation = activation  #activation function\n",
    "        self.hidden = hidden  #number of hidden layers\n",
    "        self.num_units = num_units  #number of units in the hidden layer\n",
    "        self.bias = bias #bias terms\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    #Create a network by creating an array of layers, all of which are connected with each other\n",
    "    def initialize_network(self, input_length, num_classes):\n",
    "        self.network = []\n",
    "        #Create the input layer\n",
    "        self.network.append(Layer(self.activation, input_length, self.num_units, self.bias))\n",
    "        #Add as many hidden layers as specified\n",
    "        for i in range(self.hidden - 1):\n",
    "            self.network.append(Layer(self.activation, self.num_units, self.num_units, self.bias))\n",
    "        #Add the output layer\n",
    "        self.network.append(Layer(self.softmax, self.num_units, num_classes, self.bias))\n",
    "\n",
    "    def fit(self, X, y, optimizer, num_classes=len(class_names)):\n",
    "        #Initialize the network\n",
    "        N, D = X.shape\n",
    "        self.initialize_network(D, num_classes)\n",
    "\n",
    "        def gradient(x, y, params):\n",
    "            dparams = []\n",
    "            dbiases = []\n",
    "            #Predict output on input x, save the output of each layer\n",
    "            input = x\n",
    "            forward_passes = []\n",
    "            for i, param in enumerate(params):\n",
    "                forward_passes.append(self.activation(np.dot(input, param)))\n",
    "                input = forward_passes[-1]\n",
    "            yh = forward_passes[-1]\n",
    "            dy = yh - y\n",
    "            dz = np.dot(dy, params[-1].T)\n",
    "\n",
    "            dw = np.dot(forward_passes[-2].T, dy)/N\n",
    "            db = np.sum(dz.T, axis=1, keepdims=True)/N\n",
    "\n",
    "            dparams.append(dw)\n",
    "            dbiases.append(db)\n",
    "\n",
    "            for i in reversed(range(1, len(forward_passes) - 1)):\n",
    "                dweight = np.dot(forward_passes[i - 1].T, np.multiply(dz, self.activation_prime(forward_passes[i])))/N\n",
    "                dparams.append(dweight)\n",
    "                dz = np.dot(dz, params[i].T)\n",
    "                dbiases.append(np.sum(dz.T, axis=1, keepdims=True)/N)\n",
    "\n",
    "            dweight = np.dot(x.T, np.multiply(dz, self.activation_prime(forward_passes[0])))/N\n",
    "            dparams.append(dweight)\n",
    "\n",
    "            return [dparam for dparam in reversed(dparams)], [dbias for dbias in reversed(dbiases)]\n",
    "\n",
    "        params0 = [layer.weights for layer in self.network]\n",
    "\n",
    "        params, biases = optimizer.run(gradient, X, y, params0, biases=[self.bias] * len(params0))\n",
    "\n",
    "\n",
    "        for i, layer in enumerate(self.network):\n",
    "            layer.update_weights(params[i])\n",
    "            #layer.update_biases(np.ndarray.flatten(biases[i]))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    #Predict the output of the network given input by running the input through all the layers of the network sequentially\n",
    "    def predict(self, X):\n",
    "        input = X\n",
    "\n",
    "        for layer in self.network:\n",
    "            print(\"INPUT\", input)\n",
    "            output = layer.forward(input)\n",
    "            print(\"OUTPUT\", output)\n",
    "            input = output\n",
    "\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOsxurDAIX0C"
   },
   "outputs": [],
   "source": [
    "relu = lambda v: np.maximum(0, v)\n",
    "relu_leaky = lambda v: np.where(v > 0, v, v*0.01)\n",
    "relu_leaky_prime = lambda v: np.where(v > 0, 1, 0.01)\n",
    "relu_prime = lambda v: np.where(v > 0, 1, 0)\n",
    "tanh = lambda x: (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "tanh_prime = lambda x: 1 - np.square(tanh(x))\n",
    "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
    "\n",
    "# optimizer = GradientDescent()\n",
    "#weights = np.random.rand(train_images_mean.shape[0], train_images_mean.shape[1])\n",
    "# mlp = MLP(activation = relu, hidden = 0, unit = 64, bias = .1)\n",
    "# yh = mlp.fit(train_images_mean, train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "one_hot_encode_train_labels = np.zeros((train_labels.size, train_labels.max()+1))\n",
    "one_hot_encode_train_labels[np.arange(train_labels.size),train_labels] = 1\n",
    "\n",
    "#784\n",
    "model = MLP(activation = relu, activation_prime=relu_prime, hidden = 2, num_units = 128, bias = .1)\n",
    "optimizer = GradientDescent(learning_rate=.1, max_iters=20)\n",
    "yh = model.fit(train_images_normal, one_hot_encode_train_labels, optimizer).predict(test_images_normal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3tXtZlNIX0D"
   },
   "outputs": [],
   "source": [
    "def evaluate_acc(y_true, y_pred):\n",
    "    count = 0\n",
    "    for i in range(len(y_pred)):\n",
    "      if y_true[i] == y_pred[i]:\n",
    "        count = count + 1\n",
    "    return count / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guKSJsmHIX0D"
   },
   "outputs": [],
   "source": [
    "print(test_labels)\n",
    "print([np.argmax(t) for t in yh])\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p = np.array([[-2, -2], [-2, -2], [2, -1], [3, 5]])\n",
    "print(p.shape)\n",
    "print(np.where(p > 0, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(one_hot_encode_train_labels)\n",
    "print(yh)\n",
    "print([np.argmax(t) + 1 for t in yh])\n",
    "print(test_labels)\n",
    "print(yh[0])\n",
    "print(np.argmax(yh[0]))\n",
    "print(np.where(yh[0] == np.amax(yh[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(yh)\n",
    "print(train_labels)\n",
    "print(one_hot_encode_train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(train_images_mean)\n",
    "print(train_labels)\n",
    "print(test_images_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = GradientDescent(learning_rate=.01, max_iters=2000)\n",
    "\n",
    "#Task 3: Experiment 1\n",
    "\n",
    "#hidden layer = 0\n",
    "mlp_no_hidden = MLP(activation = relu, activation_prime=relu_prime, hidden = 0, num_units = 64, bias = .1)\n",
    "\n",
    "yh_1_hidden0 = mlp_no_hidden.fit(train_images_mean, one_hot_encode_train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "def evaluate_acc(y_true, y_pred):\n",
    "    count = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            count = count + 1\n",
    "    return count / len(y_true)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh_1_hidden0]))\n",
    "yh = mlp_no_hidden.fit(train_images_mean, one_hot_encode_train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh]))\n",
    "\n",
    "predicted_classes = [np.argmax(t) for t in yh]\n",
    "\n",
    "p = predicted_classes[:10000]\n",
    "y = test_labels[:10000]\n",
    "correct = np.nonzero(p==y)[0]\n",
    "incorrect = np.nonzero(p!=y)[0]\n",
    "print(\"Correct predicted classes:\",correct.shape[0])\n",
    "print(\"Incorrect predicted classes:\",incorrect.shape[0])\n",
    "\n",
    "target_names = [\"Class {} ({}) :\".format(i,class_names[i]) for i in range(10)]\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))\n",
    "\n",
    "#hidden layer = 1\n",
    "mlp_single_layer = MLP(activation = relu, activation_prime=relu_prime, hidden = 1, num_units = 128, bias = .1)\n",
    "\n",
    "yh_1_hidden1 = mlp_single_layer.fit(train_images_mean, one_hot_encode_train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh_1_hidden1]))\n",
    "yh = mlp_single_layer.fit(train_images_mean, one_hot_encode_train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh]))\n",
    "\n",
    "predicted_classes = [np.argmax(t) for t in yh]\n",
    "\n",
    "p = predicted_classes[:10000]\n",
    "y = test_labels[:10000]\n",
    "correct = np.nonzero(p==y)[0]\n",
    "incorrect = np.nonzero(p!=y)[0]\n",
    "print(\"Correct predicted classes:\",correct.shape[0])\n",
    "print(\"Incorrect predicted classes:\",incorrect.shape[0])\n",
    "\n",
    "target_names = [\"Class {} ({}) :\".format(i,class_names[i]) for i in range(10)]\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))\n",
    "\n",
    "#hidden layer = 2\n",
    "mlp_double_layer = MLP(activation = relu, activation_prime=relu_prime, hidden = 2, num_units = 128, bias = .1)\n",
    "\n",
    "yh_1_hidden2 = mlp_double_layer.fit(train_images_mean, one_hot_encode_train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh_1_hidden2]))\n",
    "yh = mlp_double_layer.fit(train_images_mean, one_hot_encode_train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh]))\n",
    "\n",
    "predicted_classes = [np.argmax(t) for t in yh]\n",
    "\n",
    "p = predicted_classes[:10000]\n",
    "y = test_labels[:10000]\n",
    "correct = np.nonzero(p==y)[0]\n",
    "incorrect = np.nonzero(p!=y)[0]\n",
    "print(\"Correct predicted classes:\",correct.shape[0])\n",
    "print(\"Incorrect predicted classes:\",incorrect.shape[0])\n",
    "\n",
    "print(\"Experiment 1: hidden layers = 2\")\n",
    "target_names = [\"Class {} ({}) :\".format(i,class_names[i]) for i in range(10)]\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))\n",
    "\n",
    "#Task 3: Experiemnt 2\n",
    "print(\"starting experiment 2\")\n",
    "mlp_tanh_copy = MLP(activation = tanh, activation_prime=tanh_prime, hidden = 2, num_units = 128, bias = .1)\n",
    "\n",
    "yh_2_tanh = mlp_tanh_copy.fit(train_images_mean, one_hot_encode_train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh_2_tanh]))\n",
    "yh = mlp_tanh_copy.fit(train_images_mean, one_hot_encode_train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh]))\n",
    "\n",
    "predicted_classes = [np.argmax(t) for t in yh]\n",
    "\n",
    "p = predicted_classes[:10000]\n",
    "y = test_labels[:10000]\n",
    "correct = np.nonzero(p==y)[0]\n",
    "incorrect = np.nonzero(p!=y)[0]\n",
    "print(\"Correct predicted classes:\",correct.shape[0])\n",
    "print(\"Incorrect predicted classes:\",incorrect.shape[0])\n",
    "\n",
    "target_names = [\"Class {} ({}) :\".format(i,class_names[i]) for i in range(10)]\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))\n",
    "\n",
    "mlp_relu_leaky_copy = MLP(activation = relu_leaky, activation_prime=relu_leaky_prime, hidden = 2, num_units = 128, bias = .1)\n",
    "\n",
    "yh_2_relu_leaky = mlp_relu_leaky_copy.fit(train_images_mean, one_hot_encode_train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh_2_relu_leaky]))\n",
    "yh = mlp_relu_leaky_copy.fit(train_images_mean, one_hot_encode_train_labels, optimizer).predict(test_images_mean)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh]))\n",
    "\n",
    "predicted_classes = [np.argmax(t) for t in yh]\n",
    "\n",
    "p = predicted_classes[:10000]\n",
    "y = test_labels[:10000]\n",
    "correct = np.nonzero(p==y)[0]\n",
    "incorrect = np.nonzero(p!=y)[0]\n",
    "print(\"Correct predicted classes:\",correct.shape[0])\n",
    "print(\"Incorrect predicted classes:\",incorrect.shape[0])\n",
    "\n",
    "target_names = [\"Class {} ({}) :\".format(i,class_names[i]) for i in range(10)]\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))\n",
    "\n",
    "\n",
    "#Task 3: Experiment 4\n",
    "mlp_relu_unnormalized = MLP(activation = relu, hidden = 2, activation_prime=relu_prime, num_units = 128, bias = .1)\n",
    "\n",
    "train_images.reshape(60000, 28*28)\n",
    "\n",
    "yh_4 = mlp_relu_unnormalized.fit(train_images, one_hot_encode_train_labels, optimizer).predict(test_images)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh_4]))\n",
    "yh = mlp_relu_unnormalized.fit(train_images, one_hot_encode_train_labels, optimizer).predict(test_images)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh]))\n",
    "\n",
    "predicted_classes = [np.argmax(t) for t in yh]\n",
    "\n",
    "p = predicted_classes[:10000]\n",
    "y = test_labels[:10000]\n",
    "correct = np.nonzero(p==y)[0]\n",
    "incorrect = np.nonzero(p!=y)[0]\n",
    "print(\"Correct predicted classes:\",correct.shape[0])\n",
    "print(\"Incorrect predicted classes:\",incorrect.shape[0])\n",
    "\n",
    "target_names = [\"Class {} ({}) :\".format(i,class_names[i]) for i in range(10)]\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))\n",
    "\n",
    "mlp_relu_normalized = MLP(activation = relu, hidden = 2, activation_prime=relu_prime, num_units = 128, bias = .1)\n",
    "\n",
    "yh_4_normal = mlp_relu_normalized.fit(train_images_normal, one_hot_encode_train_labels, optimizer).predict(test_images_normal)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh_4_normal]))\n",
    "yh = mlp_relu_normalized.fit(train_images_normal, one_hot_encode_train_labels, optimizer).predict(test_images_normal)\n",
    "\n",
    "print(evaluate_acc(test_labels, [np.argmax(t) for t in yh]))\n",
    "\n",
    "predicted_classes = [np.argmax(t) for t in yh]\n",
    "\n",
    "p = predicted_classes[:10000]\n",
    "y = test_labels[:10000]\n",
    "correct = np.nonzero(p==y)[0]\n",
    "incorrect = np.nonzero(p!=y)[0]\n",
    "print(\"Correct predicted classes:\",correct.shape[0])\n",
    "print(\"Incorrect predicted classes:\",incorrect.shape[0])\n",
    "\n",
    "target_names = [\"Class {} ({}) :\".format(i,class_names[i]) for i in range(10)]\n",
    "print(classification_report(test_labels, predicted_classes, target_names=target_names))\n",
    "\n",
    "#Task 3: Experiment 5\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "# Add convolution 2D\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "assert train_images.shape == (60000, 28, 28)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "\n",
    "train_images, validation_images, train_labels, validation_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=2018)\n",
    "\n",
    "print(\"Fashion MNIST train -  rows:\",train_images.shape[0],\" columns:\", train_images.shape[1:4])\n",
    "print(\"Fashion MNIST valid -  rows:\",validation_images.shape[0],\" columns:\", validation_images.shape[1:4])\n",
    "print(\"Fashion MNIST test -  rows:\",test_images.shape[0],\" columns:\", test_images.shape[1:4])\n",
    "\n",
    "\n",
    "train_model = model.fit(train_images, train_labels,\n",
    "                  batch_size=1, \n",
    "                  epochs=3,\n",
    "                  validation_data=(validation_images, validation_labels))\n",
    "\n",
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MiniProject3.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "0d7944f1888d65f5965a0f83664546a5eb737a19f1d30a41383683ae88994718"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
